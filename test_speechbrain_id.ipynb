{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Speaker Recognition with SpeechBrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import speechbrain as sb\n",
    "import torch\n",
    "import torchaudio\n",
    "from moviepy.editor import VideoFileClip\n",
    "from hyperpyyaml.core import load_hyperpyyaml\n",
    "from IPython.display import Audio, display\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from speechbrain.dataio.dataset import DynamicItemDataset\n",
    "from speechbrain.dataio.encoder import CategoricalEncoder\n",
    "from speechbrain.dataio.dataloader import SaveableDataLoader\n",
    "from speechbrain.lobes.models.ECAPA_TDNN import ECAPA_TDNN\n",
    "from speechbrain.pretrained import EncoderClassifier, VAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"speaker_id_debate_21\"\n",
    "VIDEO_DIR = \"video_camera_shots\"\n",
    "AUDIO_DIR = \"audio_files\"\n",
    "DATA_DIR = \"datasets\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [\"Rutte\", \"Wilders\", \"Hoekstra\", \"Marijnissen\", \"Klaver\", \"Kaag\"]\n",
    "\n",
    "coding_df = pd.read_csv(f\"{SAVE_DIR}/camera_shots_coding.csv\", sep=\";\")\n",
    "\n",
    "random.seed(123484)\n",
    "\n",
    "shots_selected = coding_df[coding_df.face_1.isin(\n",
    "    candidates)].groupby(\"face_1\").sample(20).reset_index()\n",
    "\n",
    "shots_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_video_to_audio(shots_df, path, out_dir, force=False, replace=False):\n",
    "    if replace:\n",
    "        os.system(f\"del {out_dir}\\\\*.wav\")\n",
    "\n",
    "    for i, shot in shots_df.iterrows():\n",
    "        batch = int(np.ceil(shot.shot_id/50))\n",
    "        infile_name = f'{path}\\\\batch{batch}\\\\{shot.filename}.mp4'\n",
    "        outfile_name = f\"{out_dir}\\\\batch_{batch}_shot_{int(shot.shot_id)}_{shot.face_1}.wav\"\n",
    "\n",
    "        if not os.path.isfile(outfile_name) or force:\n",
    "            with VideoFileClip(infile_name) as clip:\n",
    "                clip.audio.write_audiofile(outfile_name, fps=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_video_to_audio(\n",
    "    shots_selected, f\"{SAVE_DIR}\\\\{VIDEO_DIR}\", f\"{SAVE_DIR}\\\\{AUDIO_DIR}\", replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid_test_set(path, frac=0.1):\n",
    "    with os.scandir(path) as sc:\n",
    "        filenames = [filename.name for filename in sc if filename.is_file()]\n",
    "\n",
    "    filenames_df = pd.DataFrame(filenames)\n",
    "    filenames_df[\"name\"] = filenames_df[0].str.split(\"_\").str[-1]\n",
    "\n",
    "    train_set = []\n",
    "    valid_set = []\n",
    "    test_set = []\n",
    "\n",
    "    for group in filenames_df.groupby(\"name\"):\n",
    "        train = group[1][0].to_list()\n",
    "        k = len(train)\n",
    "        valid = [train.pop(i) for i in random.sample(\n",
    "            range(len(train)), k=int(k*frac))]\n",
    "        test = [train.pop(i) for i in random.sample(\n",
    "            range(len(train)), k=int(k*frac))]\n",
    "        [train_set.append(i) for i in train]\n",
    "        [valid_set.append(i) for i in valid]\n",
    "        [test_set.append(i) for i in test]\n",
    "\n",
    "    return train_set, valid_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12314)\n",
    "\n",
    "train_set, valid_set, test_set = split_train_valid_test_set(\n",
    "    f\"{SAVE_DIR}/{AUDIO_DIR}\")\n",
    "\n",
    "print(len(test_set), len(valid_set), len(train_set))\n",
    "print(test_set, \"\\n\\n\", valid_set, \"\\n\\n\", train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_train_valid_test_files(train_set, valid_set, test_set, path, out_dir, replace=True):\n",
    "    if replace:\n",
    "        os.system(f\"del {out_dir}\\\\training\\\\*.wav\")\n",
    "        os.system(f\"del {out_dir}\\\\validation\\\\*.wav\")\n",
    "        os.system(f\"del {out_dir}\\\\test\\\\*.wav\")\n",
    "\n",
    "    for filename in train_set:\n",
    "        os.system(f\"copy {path}\\\\{filename} {out_dir}\\\\training\\\\\")\n",
    "\n",
    "    for filename in valid_set:\n",
    "        os.system(f\"copy {path}\\\\{filename} {out_dir}\\\\validation\\\\\")\n",
    "\n",
    "    for filename in test_set:\n",
    "        os.system(f\"copy {path}\\\\{filename} {out_dir}\\\\test\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_train_valid_test_files(train_set, valid_set, test_set,\n",
    "                            f\"{SAVE_DIR}\\\\{AUDIO_DIR}\", f\"{SAVE_DIR}\\\\{DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_annotation_file(data_dir, out_file, force=True):\n",
    "    \"\"\" Creates a data annotation file in .json format with three fiels:\n",
    "            \"file_path\": Path to the sound file\n",
    "            \"spk_id\": Name of the speaker\n",
    "            \"length\": Length of the sound signal (frames)\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(out_file) or force:\n",
    "        with os.scandir(data_dir) as sc:\n",
    "            filenames = [\n",
    "                filename.name for filename in sc if filename.is_file()]\n",
    "\n",
    "        annotation_dict = {}\n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            new_id = str(i)\n",
    "            new_path = data_dir + filename\n",
    "            new_spk = filename.split(\"_\")[-1].split(\".\")[0]\n",
    "            new_signal, _ = torchaudio.load(new_path)\n",
    "            new_length = new_signal.shape[1]\n",
    "            new_dict = {\n",
    "                \"file_path\": new_path,\n",
    "                \"spk_id\": new_spk,\n",
    "                \"length\": new_length\n",
    "            }\n",
    "            annotation_dict[new_id] = new_dict\n",
    "\n",
    "        with open(out_file, \"w\") as file:\n",
    "            file.write(json.dumps(annotation_dict))\n",
    "\n",
    "        print(f\"Created data annotation file at {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_annotation_files(save_dir, data_dir_names, force=True):\n",
    "    \"\"\" Creates data annotation files for multiple directories \"\"\"\n",
    "    for data_dir in data_dir_names:\n",
    "        create_data_annotation_file(\n",
    "            save_dir + \"/\" + data_dir + \"/\", f\"{save_dir}/{data_dir}.json\", force=force)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data_annotation_files(\n",
    "    f\"{SAVE_DIR}/{DATA_DIR}\", [\"training\", \"validation\", \"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_voice_activation(signal, filename, vad):\n",
    "    \"\"\"\n",
    "    Extracts voice activation (speech) segments from an audio signal using a neural VAD model:\n",
    "    Computes posterior probability for speech segments from neural VAD model.\n",
    "    Applies a threshold on the posterior probability to get candidate segments.\n",
    "    Extracts speech segments using energy-based VAD.\n",
    "    Merges segments that are close to each other.\n",
    "    Removes short segments.\n",
    "    Double check the energy-based VAD using the neural VAD model.\n",
    "    \"\"\"\n",
    "    prob_chunks = vad.get_speech_prob_chunk(signal)\n",
    "    prob_chunks_avg = prob_chunks.mean(dim=0, keepdim=True)\n",
    "    prob_th = vad.apply_threshold(\n",
    "        prob_chunks_avg, activation_th=0.5, deactivation_th=0.25).float()\n",
    "    boundaries = vad.get_boundaries(prob_th)\n",
    "    boundaries_energy = vad.energy_VAD(\n",
    "        filename, boundaries, activation_th=0.8, deactivation_th=0.0)\n",
    "    boundaries_merged = vad.merge_close_segments(\n",
    "        boundaries_energy, close_th=0.250)\n",
    "    boundaries_short_removed = vad.remove_short_segments(\n",
    "        boundaries_merged, len_th=0.250)\n",
    "    boundaries_checked = vad.double_check_speech_segments(\n",
    "        boundaries_short_removed, filename, speech_th=0.5)\n",
    "\n",
    "    return vad.upsample_boundaries(boundaries, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataio(filename, save_dir, training=False):\n",
    "    \"\"\" Prepares data sets for the Brain class.\n",
    "        Encodes speaker names and saves the encoding.\n",
    "    \"\"\"\n",
    "    spk_id_encoder = CategoricalEncoder()\n",
    "\n",
    "    vad = VAD.from_hparams(source=\"speechbrain/vad-crdnn-libriparty\",\n",
    "                           savedir=\"pretrained_models/vad-crdnn-libriparty\")\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"file_path\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(file_path):\n",
    "        sig, _ = torchaudio.load(file_path)\n",
    "        vad_sig = detect_voice_activation(sig, file_path, vad)\n",
    "        return torch.masked_select(sig, vad_sig.bool())\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"spk_id\")\n",
    "    @sb.utils.data_pipeline.provides(\"spk_id\", \"spk_id_encoded\")\n",
    "    def label_pipeline(spk_id):\n",
    "        yield spk_id\n",
    "        spk_id_encoded = torch.LongTensor(\n",
    "            [spk_id_encoder.encode_label(spk_id)])\n",
    "        yield spk_id_encoded\n",
    "\n",
    "    dataset = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "        json_path=filename,\n",
    "        dynamic_items=[audio_pipeline, label_pipeline],\n",
    "        output_keys=[\"id\", \"sig\", \"spk_id_encoded\"],\n",
    "    )\n",
    "\n",
    "    if training:\n",
    "        spk_id_encoder.update_from_didataset(dataset, output_key=\"spk_id\")\n",
    "        spk_id_encoder.save(f\"{save_dir}/spk_id_encoder.txt\")\n",
    "    else:\n",
    "        spk_id_encoder.load(f\"{save_dir}/spk_id_encoder.txt\")\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(save_dir, data_dirs):\n",
    "    \"\"\" Prepares dataset for multiple directories \"\"\"\n",
    "    datasets = {}\n",
    "    for data_dir in data_dirs:\n",
    "        datasets[data_dir] = prepare_dataio(\n",
    "            f\"{save_dir}/{data_dir}.json\", save_dir, data_dir == \"training\")\n",
    "\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = prepare_datasets(\n",
    "    f\"{SAVE_DIR}/{DATA_DIR}\", [\"training\", \"validation\", \"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecapa = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"./pretrained_models/ecapa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [ecapa.encode_batch(batch[\"sig\"]).squeeze()\n",
    "              for batch in datasets[\"training\"]]\n",
    "speakers = [batch[\"spk_id_encoded\"] for batch in datasets[\"training\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline.fit(pd.DataFrame(np.array([e.numpy() for e in embeddings])), [\n",
    "                int(s) for s in speakers])\n",
    "ml_pipeline.score(pd.DataFrame(np.array([e.numpy() for e in embeddings])), [\n",
    "                  int(s) for s in speakers])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_valid = [ecapa.encode_batch(\n",
    "    batch[\"sig\"]).squeeze() for batch in datasets[\"validation\"]]\n",
    "speakers_valid = [batch[\"spk_id_encoded\"] for batch in datasets[\"validation\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ml_pipeline.predict(pd.DataFrame(\n",
    "    np.array([e.numpy() for e in embeddings_valid])))\n",
    "acc = ml_pipeline.score(pd.DataFrame(np.array(\n",
    "    [e.numpy() for e in embeddings_valid])), [int(s) for s in speakers_valid])\n",
    "print([int(s) for s in speakers_valid], preds, acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test = [ecapa.encode_batch(\n",
    "    batch[\"sig\"]).squeeze() for batch in datasets[\"test\"]]\n",
    "speakers_test = [batch[\"spk_id_encoded\"] for batch in datasets[\"test\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ml_pipeline.predict(pd.DataFrame(\n",
    "    np.array([e.numpy() for e in embeddings_test])))\n",
    "acc = ml_pipeline.score(pd.DataFrame(np.array(\n",
    "    [e.numpy() for e in embeddings_test])), [int(s) for s in speakers_test])\n",
    "print([int(s) for s in speakers_test], preds, acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpkIdBrain(sb.Brain):\n",
    "    \"\"\" New speaker recognition class that inherits from Brain base class.\n",
    "        Requires at least compute_forward() and compute_objective() methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Runs all the computation of that transforms the input into the\n",
    "        output probabilities over the N classes.\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : Tensor\n",
    "            Tensor that contains the posterior probabilities over the N classes.\n",
    "        \"\"\"\n",
    "\n",
    "        # We first move the batch to the appropriate device.\n",
    "        batch = batch.to(self.device)\n",
    "\n",
    "        # Compute features, embeddings, and predictions\n",
    "        feats, lens = self.prepare_features(batch.sig, stage)\n",
    "        embeddings = self.modules.embedding_model(feats, lens)\n",
    "        predictions = self.modules.classifier(embeddings)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def prepare_features(self, wavs, stage):\n",
    "        \"\"\" Prepare the features for computation, including augmentation.\n",
    "        Arguments\n",
    "        ---------\n",
    "        wavs : tuple\n",
    "            Input signals (tensor) and their relative lengths (tensor).\n",
    "        stage : sb.Stage\n",
    "            The current stage of training.\n",
    "        \"\"\"\n",
    "        wavs, lens = wavs\n",
    "\n",
    "        # Feature extraction and normalization\n",
    "        feats = self.modules.compute_features(wavs)\n",
    "        feats = self.modules.mean_var_norm(feats, lens)\n",
    "\n",
    "        return feats, lens\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\" Computes the loss given the predicted and targeted outputs.\n",
    "        Arguments\n",
    "        ---------\n",
    "        predictions : tensor\n",
    "            The output tensor from `compute_forward`.\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            A one-element tensor used for backpropagating the gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        _, lens = batch.sig\n",
    "        spkid, _ = batch.spk_id_encoded\n",
    "\n",
    "        # Concatenate labels (due to data augmentation)\n",
    "        if stage == sb.Stage.TRAIN and hasattr(self.modules, \"env_corrupt\"):\n",
    "            spkid = torch.cat([spkid, spkid], dim=0)\n",
    "            lens = torch.cat([lens, lens])\n",
    "\n",
    "        # Compute the cost function\n",
    "        loss = self.hparams.compute_cost(predictions, spkid, lens)\n",
    "\n",
    "        # Append this batch of losses to the loss metric for easy\n",
    "        self.loss_metric.append(\n",
    "            batch.id, predictions, spkid, lens, reduction=\"batch\"\n",
    "        )\n",
    "\n",
    "        # Compute classification error at test time\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics.append(batch.id, predictions, spkid, lens)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_stage_start(self, stage, epoch=None):\n",
    "        \"\"\" Gets called at the beginning of each epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set up statistics trackers for this stage\n",
    "        self.loss_metric = sb.utils.metric_stats.MetricStats(\n",
    "            metric=sb.nnet.losses.nll_loss\n",
    "        )\n",
    "\n",
    "        # Set up evaluation-only statistics trackers\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics = self.hparams.error_stats()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
    "        \"\"\" Gets called at the end of an epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
    "        stage_loss : float\n",
    "            The average loss for all of the data processed in this stage.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the train loss until the validation stage.\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_loss = stage_loss\n",
    "\n",
    "        # Summarize the statistics from the stage for record-keeping.\n",
    "        else:\n",
    "            stats = {\n",
    "                \"loss\": stage_loss,\n",
    "                \"error\": self.error_metrics.summarize(\"average\"),\n",
    "            }\n",
    "\n",
    "        # At the end of validation...\n",
    "        if stage == sb.Stage.VALID:\n",
    "\n",
    "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
    "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
    "\n",
    "            # The train_logger writes a summary to stdout and to the logfile.\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\"Epoch\": epoch, \"lr\": old_lr},\n",
    "                train_stats={\"loss\": self.train_loss},\n",
    "                valid_stats=stats,\n",
    "            )\n",
    "\n",
    "            # Save the current checkpoint and delete previous checkpoints,\n",
    "            # self.checkpointer.save_and_keep_only(meta=stats, min_keys=[\"error\"])\n",
    "\n",
    "        # We also write statistics about test data to stdout and to the logfile.\n",
    "        if stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats=stats,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{SAVE_DIR}/hyperparams_pre.yaml\") as file:\n",
    "    hparams = load_hyperpyyaml(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_id_brain = SpkIdBrain(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"opt_class\"],\n",
    "    hparams=hparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_id_brain.fit(\n",
    "    epoch_counter=range(5),\n",
    "    train_set=datasets[\"training\"],\n",
    "    valid_set=datasets[\"validation\"],\n",
    "    train_loader_kwargs=hparams[\"dataloader_options\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stat = spk_id_brain.evaluate(\n",
    "    test_set=datasets[\"test\"],\n",
    "    min_key=\"error\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mexca-exploration",
   "language": "python",
   "name": "mexca-exploration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
