{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Speaker Diarization with SpeechBrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Explore speechbrain speaker diarization \"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import speechbrain as sb\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, normalized_mutual_info_score, v_measure_score\n",
    "from speechbrain.pretrained import EncoderClassifier, VAD\n",
    "from speechbrain.processing.PLDA_LDA import StatObject_SB\n",
    "from speechbrain.processing import diarization as diar\n",
    "from speechbrain.dataio.encoder import CategoricalEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"speaker_id_debate_21\"\n",
    "VIDEO_DIR = \"video_camera_shots\"\n",
    "AUDIO_DIR = \"audio_files\"\n",
    "DATA_DIR = \"datasets\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_voice_activation(signal, filename, vad):\n",
    "    \"\"\"\n",
    "    Extracts voice activation (speech) segments from an audio signal using a neural VAD model:\n",
    "    Computes posterior probability for speech segments from neural VAD model.\n",
    "    Applies a threshold on the posterior probability to get candidate segments.\n",
    "    Extracts speech segments using energy-based VAD.\n",
    "    Merges segments that are close to each other.\n",
    "    Removes short segments.\n",
    "    Double check the energy-based VAD using the neural VAD model.\n",
    "    \"\"\"\n",
    "    prob_chunks = vad.get_speech_prob_chunk(signal)\n",
    "    prob_chunks_avg = prob_chunks.mean(dim=0, keepdim=True)\n",
    "    prob_th = vad.apply_threshold(\n",
    "        prob_chunks_avg, activation_th=0.5, deactivation_th=0.25).float()\n",
    "    boundaries = vad.get_boundaries(prob_th)\n",
    "    boundaries_energy = vad.energy_VAD(\n",
    "        filename, boundaries, activation_th=0.8, deactivation_th=0.0)\n",
    "    boundaries_merged = vad.merge_close_segments(\n",
    "        boundaries_energy, close_th=0.250)\n",
    "    boundaries_short_removed = vad.remove_short_segments(\n",
    "        boundaries_merged, len_th=0.250)\n",
    "    boundaries_checked = vad.double_check_speech_segments(\n",
    "        boundaries_short_removed, filename, speech_th=0.5)\n",
    "\n",
    "    return vad.upsample_boundaries(boundaries, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataio(filename, save_dir, training=False):\n",
    "    \"\"\" Prepares data sets for the Brain class.\n",
    "        Encodes speaker names and saves the encoding.\n",
    "    \"\"\"\n",
    "    spk_id_encoder = CategoricalEncoder()\n",
    "\n",
    "    vad = VAD.from_hparams(source=\"speechbrain/vad-crdnn-libriparty\",\n",
    "                           savedir=\"pretrained_models/vad-crdnn-libriparty\")\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"file_path\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(file_path):\n",
    "        sig, _ = torchaudio.load(file_path)\n",
    "        vad_sig = detect_voice_activation(sig, file_path, vad)\n",
    "        return torch.masked_select(sig, vad_sig.bool())\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"spk_id\")\n",
    "    @sb.utils.data_pipeline.provides(\"spk_id\", \"spk_id_encoded\")\n",
    "    def label_pipeline(spk_id):\n",
    "        yield spk_id\n",
    "        spk_id_encoded = torch.LongTensor(\n",
    "            [spk_id_encoder.encode_label(spk_id)])\n",
    "        yield spk_id_encoded\n",
    "\n",
    "    dataset = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "        json_path=filename,\n",
    "        dynamic_items=[audio_pipeline, label_pipeline],\n",
    "        output_keys=[\"id\", \"sig\", \"spk_id_encoded\"],\n",
    "    )\n",
    "\n",
    "    if training:\n",
    "        spk_id_encoder.update_from_didataset(dataset, output_key=\"spk_id\")\n",
    "        spk_id_encoder.save(f\"{save_dir}/spk_id_encoder.txt\")\n",
    "    else:\n",
    "        spk_id_encoder.load(f\"{save_dir}/spk_id_encoder.txt\")\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(save_dir, data_dirs):\n",
    "    \"\"\" Prepares dataset for multiple directories \"\"\"\n",
    "    datasets = {}\n",
    "    for data_dir in data_dirs:\n",
    "        datasets[data_dir] = prepare_dataio(\n",
    "            f\"{save_dir}/{data_dir}.json\", save_dir, data_dir == \"training\")\n",
    "\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = prepare_datasets(\n",
    "    f\"{SAVE_DIR}/{DATA_DIR}\", [\"training\", \"validation\", \"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecapa = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"./pretrained_models/ecapa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [ecapa.encode_batch(batch[\"sig\"]).squeeze()\n",
    "              for batch in datasets[\"training\"]]\n",
    "\n",
    "speakers = [int(batch[\"spk_id_encoded\"]) for batch in datasets[\"training\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = np.array([e.numpy() for e in embeddings])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_obj = StatObject_SB(\n",
    "    modelset=np.array(\n",
    "        [\"EPACA_TDNN_embeddings\" * embeddings_array.shape[0]], dtype=\"|O\"),\n",
    "    segset=np.array(\n",
    "        [f\"{i}_{i}_{i}\" for i in range(embeddings_array.shape[0])], dtype=\"|O\"),\n",
    "    start=np.array([None] * embeddings_array.shape[0]),\n",
    "    stop=np.array([None] * embeddings_array.shape[0]),\n",
    "    stat0=np.array([[1.0]] * embeddings_array.shape[0]),\n",
    "    stat1=embeddings_array.squeeze()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "diar.do_spec_clustering(\n",
    "    diary_obj=stat_obj,\n",
    "    out_rttm_file=f\"{SAVE_DIR}/debate_21.rttm\",\n",
    "    rec_id=\"debate_21\",\n",
    "    k=6,\n",
    "    pval=0.025,  # This parameter can be fine-tuned\n",
    "    affinity_type=\"cos\",\n",
    "    n_neighbors=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rttm(filename):\n",
    "    \"\"\" Reads output of spectral clustering (.rttm file)\n",
    "    and create segments [start, end, id] \"\"\"\n",
    "    segments = []\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        for row in file:\n",
    "            row_split = row.split(\" \")\n",
    "            start = float(row_split[3])\n",
    "            end = float(row_split[3]) + float(row_split[4])\n",
    "            spk_id = int(row_split[7].split(\"_\")[-1])\n",
    "            segments.append([start, end, spk_id])\n",
    "\n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary = read_rttm(f\"{SAVE_DIR}/debate_21.rttm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_speakers = [row[2] for row in diary]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15518346418843135"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(speakers, diary_speakers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18631488098088309"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_mutual_info_score(speakers, diary_speakers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25665124614810164"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_measure_score(speakers, diary_speakers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mexca-exploration",
   "language": "python",
   "name": "mexca-exploration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
