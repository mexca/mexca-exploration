{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8488a86a-4744-42be-bf4b-361559836f13",
   "metadata": {},
   "source": [
    "# Explore SpeechBrain: Speaker Recognition\n",
    "\n",
    "This notebook will explore speaker recognition using the Python library [SpeechBrain](https://speechbrain.github.io/index.html). It can be installed with `pip install speechbrain`. To run the notebook for the first time, it must be opened with administrator rights.\n",
    "\n",
    "In the first part, a custom model for speaker recognition will be build from scratch. In the second part, a pretrained speaker recognition model will be fine-tuned for the example data set. The data files are snippets from speeches by US presidents Biden, Obama, and Trump (from [this](https://www.englishspeecheschannel.com/english-speeches/) website) and stored at `/speaker_id_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22725ba5-64cb-477b-8a5e-ecfe743ed998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Explore speechbrain speaker recognition \"\"\"\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import speechbrain as sb\n",
    "import torch\n",
    "import torchaudio\n",
    "from hyperpyyaml.core import load_hyperpyyaml\n",
    "from IPython.display import Audio, display\n",
    "from speechbrain.dataio.dataset import DynamicItemDataset\n",
    "from speechbrain.dataio.encoder import CategoricalEncoder\n",
    "from speechbrain.dataio.dataloader import SaveableDataLoader\n",
    "from speechbrain.dataio.batch import PaddedBatch\n",
    "from speechbrain.lobes.features import MFCC, Fbank\n",
    "from speechbrain.lobes.models.ECAPA_TDNN import ECAPA_TDNN \n",
    "from speechbrain.nnet.losses import nll_loss\n",
    "from speechbrain.utils.parameter_transfer import Pretrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33492800-7726-4504-9722-f4c2ad52a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"speaker_id_model/\"\n",
    "DATA_DIRS = [\"training\", \"validation\", \"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32181a03-9be9-4494-b63a-8258d4e5adcd",
   "metadata": {},
   "source": [
    "To fit SpeechBrain models to data, it is best to create a data loader pipeline. This pipeline requires a data annotation file (in `.json` or `.csv` format) that contains the metadata for the audio files. In this case, the metadata fields for the file path, speaker name, and signal length are specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c40d5043-4c4a-4eda-a2cc-227f49706db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_annotation_file(data_dir, out_file, force=False):\n",
    "    \"\"\" Creates a data annotation file in .json format with three fiels:\n",
    "            \"file_path\": Path to the sound file\n",
    "            \"spk_id\": Name of the speaker\n",
    "            \"length\": Length of the sound signal (frames)\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(out_file) or force:\n",
    "        with os.scandir(data_dir) as sc:\n",
    "            filenames = [\n",
    "                filename.name for filename in sc if filename.is_file() and len(filename.name.split(\"_\")) == 3]\n",
    "\n",
    "        annotation_dict = {}\n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            new_id = str(i)\n",
    "            new_path = data_dir + filename\n",
    "            new_spk = filename.split(\"_\")[0]\n",
    "            new_signal, _ = torchaudio.load(new_path)\n",
    "            new_length = new_signal.shape[1]\n",
    "            new_dict = {\n",
    "                \"file_path\": new_path,\n",
    "                \"spk_id\": new_spk,\n",
    "                \"length\": new_length\n",
    "            }\n",
    "            annotation_dict[new_id] = new_dict\n",
    "\n",
    "        with open(out_file, \"w\") as file:\n",
    "            file.write(json.dumps(annotation_dict))\n",
    "\n",
    "        print(f\"Created data annotation file at {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd9b38e-6a8d-4474-901f-1d0324217fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_annotation_files(save_dir, data_dir_names, force=False):\n",
    "    \"\"\" Creates data annotation files for multiple directories \"\"\"\n",
    "    for data_dir in data_dir_names:\n",
    "        create_data_annotation_file(\n",
    "            save_dir + data_dir + \"/\", f\"{save_dir}/{data_dir}.json\", force=force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe497091-aab0-445a-a54d-a03847a0e980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data annotation file at speaker_id_model//training.json\n",
      "Created data annotation file at speaker_id_model//validation.json\n",
      "Created data annotation file at speaker_id_model//test.json\n"
     ]
    }
   ],
   "source": [
    "create_data_annotation_files(SAVE_DIR, DATA_DIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db4b26e-5644-4b39-b652-66e0b2ff701b",
   "metadata": {},
   "source": [
    "The data loader function reads the metadata from the annotation file and combines them with the audio signal, returning a readable dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac261f3b-849f-4351-bbe6-97eb0d4dd8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataio(filename, save_dir, training=False):\n",
    "    \"\"\" Prepares data sets for the Brain class.\n",
    "        Encodes speaker names and saves the encoding.\n",
    "    \"\"\"\n",
    "    spk_id_encoder = CategoricalEncoder()\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"file_path\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(file_path):\n",
    "        sig = sb.dataio.dataio.read_audio(file_path)\n",
    "        return sig\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"spk_id\")\n",
    "    @sb.utils.data_pipeline.provides(\"spk_id\", \"spk_id_encoded\")\n",
    "    def label_pipeline(spk_id):\n",
    "        yield spk_id\n",
    "        spk_id_encoded = torch.LongTensor(\n",
    "            [spk_id_encoder.encode_label(spk_id)])\n",
    "        yield spk_id_encoded\n",
    "\n",
    "    dataset = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "        json_path=filename,\n",
    "        dynamic_items=[audio_pipeline, label_pipeline],\n",
    "        output_keys=[\"id\", \"sig\", \"spk_id_encoded\"],\n",
    "    )\n",
    "\n",
    "    if training:\n",
    "        spk_id_encoder.update_from_didataset(dataset, output_key=\"spk_id\")\n",
    "        spk_id_encoder.save(f\"{save_dir}/spk_id_encoder.txt\")\n",
    "    else:\n",
    "        spk_id_encoder.load(f\"{save_dir}/spk_id_encoder.txt\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "312d63ee-bb78-40cc-9b19-e2f85ef69bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(save_dir, data_dirs):\n",
    "    \"\"\" Prepares dataset for multiple directories \"\"\"\n",
    "    datasets = {}\n",
    "    for data_dir in data_dirs:\n",
    "        datasets[data_dir] = prepare_dataio(\n",
    "            f\"{save_dir}/{data_dir}.json\", save_dir, data_dir == \"training\")\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b3354-7da6-40ed-9366-30f4b2c0e595",
   "metadata": {},
   "source": [
    "## Training a Speaker Recognition Model from Scratch\n",
    "A new model class is created from [this](https://github.com/speechbrain/speechbrain/blob/develop/templates/speaker_id/train.py) template that will perform the speaker recognition. Several methods are added for the new class. The model has three stages:\n",
    "\n",
    "1. Computing features from audio signal\n",
    "2. Calculating embeddings from the features\n",
    "3. Classify speaker based on embeddings\n",
    "\n",
    "It uses the negative log-likelihood loss function for learning. The specification of the model is stored in the `hyperparams.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d2cf912-265f-48ad-8a63-fcf7e495736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpkIdBrain(sb.Brain):\n",
    "    \"\"\" New speaker recognition class that inherits from Brain base class.\n",
    "        Requires at least compute_forward() and compute_objective() methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Runs all the computation of that transforms the input into the\n",
    "        output probabilities over the N classes.\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : Tensor\n",
    "            Tensor that contains the posterior probabilities over the N classes.\n",
    "        \"\"\"\n",
    "\n",
    "        # We first move the batch to the appropriate device.\n",
    "        batch = batch.to(self.device)\n",
    "\n",
    "        # Compute features, embeddings, and predictions\n",
    "        feats, lens = self.prepare_features(batch.sig, stage)\n",
    "        embeddings = self.modules.embedding_model(feats, lens)\n",
    "        predictions = self.modules.classifier(embeddings)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def prepare_features(self, wavs, stage):\n",
    "        \"\"\" Prepare the features for computation, including augmentation.\n",
    "        Arguments\n",
    "        ---------\n",
    "        wavs : tuple\n",
    "            Input signals (tensor) and their relative lengths (tensor).\n",
    "        stage : sb.Stage\n",
    "            The current stage of training.\n",
    "        \"\"\"\n",
    "        wavs, lens = wavs\n",
    "\n",
    "        # Feature extraction and normalization\n",
    "        feats = self.modules.compute_features(wavs)\n",
    "        feats = self.modules.mean_var_norm(feats, lens)\n",
    "\n",
    "        return feats, lens\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\" Computes the loss given the predicted and targeted outputs.\n",
    "        Arguments\n",
    "        ---------\n",
    "        predictions : tensor\n",
    "            The output tensor from `compute_forward`.\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            A one-element tensor used for backpropagating the gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        _, lens = batch.sig\n",
    "        spkid, _ = batch.spk_id_encoded\n",
    "\n",
    "        # Concatenate labels (due to data augmentation)\n",
    "        if stage == sb.Stage.TRAIN and hasattr(self.modules, \"env_corrupt\"):\n",
    "            spkid = torch.cat([spkid, spkid], dim=0)\n",
    "            lens = torch.cat([lens, lens])\n",
    "\n",
    "        # Compute the cost function\n",
    "        loss = self.hparams.compute_cost(predictions, spkid, lens)\n",
    "\n",
    "        # Append this batch of losses to the loss metric for easy\n",
    "        self.loss_metric.append(\n",
    "            batch.id, predictions, spkid, lens, reduction=\"batch\"\n",
    "        )\n",
    "\n",
    "        # Compute classification error at test time\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics.append(batch.id, predictions, spkid, lens)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_stage_start(self, stage, epoch=None):\n",
    "        \"\"\" Gets called at the beginning of each epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set up statistics trackers for this stage\n",
    "        self.loss_metric = sb.utils.metric_stats.MetricStats(\n",
    "            metric=sb.nnet.losses.nll_loss\n",
    "        )\n",
    "\n",
    "        # Set up evaluation-only statistics trackers\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics = self.hparams.error_stats()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
    "        \"\"\" Gets called at the end of an epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
    "        stage_loss : float\n",
    "            The average loss for all of the data processed in this stage.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the train loss until the validation stage.\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_loss = stage_loss\n",
    "\n",
    "        # Summarize the statistics from the stage for record-keeping.\n",
    "        else:\n",
    "            stats = {\n",
    "                \"loss\": stage_loss,\n",
    "                \"error\": self.error_metrics.summarize(\"average\"),\n",
    "            }\n",
    "\n",
    "        # At the end of validation...\n",
    "        if stage == sb.Stage.VALID:\n",
    "\n",
    "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
    "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
    "\n",
    "            # The train_logger writes a summary to stdout and to the logfile.\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\"Epoch\": epoch, \"lr\": old_lr},\n",
    "                train_stats={\"loss\": self.train_loss},\n",
    "                valid_stats=stats,\n",
    "            )\n",
    "\n",
    "            # Save the current checkpoint and delete previous checkpoints,\n",
    "            self.checkpointer.save_and_keep_only(meta=stats, min_keys=[\"error\"])\n",
    "\n",
    "        # We also write statistics about test data to stdout and to the logfile.\n",
    "        if stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats=stats,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ddd16ec-b5c6-46ef-bdd2-5c19bd9a752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = prepare_datasets(SAVE_DIR, DATA_DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dfd1608e-f35f-4d08-b3f8-d3a6f5a10144",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{SAVE_DIR}/hyperparams.yaml\") as file:\n",
    "    hparams = load_hyperpyyaml(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1050660d-4567-43bd-b647-d6a13aa7ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_id_brain = SpkIdBrain(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"opt_class\"],\n",
    "    hparams=hparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7fef7c13-5981-4407-b174-d19d9cee2ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 6/6 [00:30<00:00,  5.02s/it, train_loss=1.16]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "spk_id_brain.fit(\n",
    "    epoch_counter=range(1),\n",
    "    train_set=datasets[\"training\"],\n",
    "    valid_set=datasets[\"validation\"],\n",
    "    train_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    valid_loader_kwargs=hparams[\"dataloader_options\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac18c2a-3c08-4a9e-8638-8eee8499da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stat = spk_id_brain.evaluate(\n",
    "    test_set=datasets[\"test\"],\n",
    "    min_key=\"error\",\n",
    "    test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89721446-6b56-46d1-a67c-e75554a5c816",
   "metadata": {},
   "source": [
    "The performance of the model is stored in `log.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7426601-33ec-4740-bb3c-627021b40a97",
   "metadata": {},
   "source": [
    "## Adapting and Finetuning a Pretrained Speaker Recognition Model\n",
    "In the second part, the pretrained embeddings from the ECAPA-TDNN model will be used for speaker recognition. The hyperparameters of this pretrained model are stored at `/speaker_id_models/hyperparams_pre.yaml`. This file also contains the model structure which is the same as in ECAPA-TDNN. To transfer the weights from the pretrained model, a `pretrainer` section is added that specifies the pretrained modules and the paths to the checkpoints of the pretrained models. The pretrained model can then be fit to the new training data to fine-tune the classifier. It can be seen that the loss and test error is substantially lower compared to the previous model that was trained from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f5674f2-b3f2-4957-aa0e-9fa339e68287",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{SAVE_DIR}/hyperparams_pre.yaml\") as file:\n",
    "    hparams_pre = load_hyperpyyaml(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "012428a0-625a-4639-a970-0202b8cca0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_id_brain_pre = SpkIdBrain(\n",
    "    modules=hparams_pre[\"modules\"],\n",
    "    opt_class=hparams_pre[\"opt_class\"],\n",
    "    hparams=hparams_pre,\n",
    "    checkpointer=hparams_pre[\"checkpointer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d31e9bed-9e0b-493a-ba87-c56d99348b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 36/36 [03:15<00:00,  5.43s/it, train_loss=4.51]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [00:11<00:00,  1.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 36/36 [04:14<00:00,  7.06s/it, train_loss=4.18]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [00:11<00:00,  1.62it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 36/36 [04:00<00:00,  6.69s/it, train_loss=4.26]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [00:10<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "spk_id_brain_pre.fit(\n",
    "    epoch_counter=range(3),\n",
    "    train_set=datasets[\"training\"],\n",
    "    valid_set=datasets[\"validation\"],\n",
    "    train_loader_kwargs=hparams_pre[\"dataloader_options\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e91d0d0-1993-4734-8a17-cd7e95a22b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [00:08<00:00,  2.19it/s]\n"
     ]
    }
   ],
   "source": [
    "test_stat = spk_id_brain_pre.evaluate(\n",
    "    test_set=datasets[\"test\"],\n",
    "    min_key=\"loss\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mexca-exploration",
   "language": "python",
   "name": "mexca-exploration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
